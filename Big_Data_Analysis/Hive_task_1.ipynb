{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this task is to create an external table on the posts data of the stackoverflow.com website.\n",
    "\n",
    "Create your own database and `use` it. \n",
    "Create external table `posts_sample_external` over the sample dataset with posts in `/data/stackexchange1000` directory. \n",
    "Create managed table `posts_sample` and populate with the data from the external table. \n",
    "`Posts_sample` table should be partitioned by year and by month of post creation. \n",
    "Provide output of query which selects lines number per each partition in the format:\n",
    "\n",
    "```\n",
    "year <tab> month <table> lines count\n",
    "```\n",
    "\n",
    "where year in YYYY format and month in YYYY-MM format. \n",
    "The result is the 3th line of the last query output.\n",
    "\n",
    "Example:\n",
    "```\n",
    "2008 2008-07 123\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "DROP DATABASE IF EXISTS stackexchange CASCADE;\n",
    "\n",
    "CREATE DATABASE stackexchange LOCATION '/data/stackexchange1000';\n",
    "\n",
    "USE stackexchange;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a query.hql\n",
    "\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "\n",
    "CREATE EXTERNAL TABLE posts_sample(\n",
    "    Id STRING,\n",
    "    PostTypeId STRING,\n",
    "    ParentId STRING,\n",
    "    CreationDate STRING\n",
    ")\n",
    "ROW FORMAT\n",
    "SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "\"input.regex\" = '(?=.*\\\\bId=\\\"(\\\\d*)\\\")?(?=.*\\\\bParentId=\\\"(\\\\S*)\\\")?(?=.*\\\\bPostTypeId=\\\"(\\\\S*)\\\")?(?=.*\\\\bCreationDate=\\\"(\\\\S*)\\\")?.*$'\n",
    ")\n",
    "LOCATION '/data/stackexchange1000/posts'\n",
    "tblproperties (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a query.hql\n",
    "\n",
    "DROP VIEW IF EXISTS posts_sample_view;\n",
    "\n",
    "CREATE VIEW posts_sample_view(\n",
    "    Id,\n",
    "    PostTypeId,\n",
    "    ParentId,\n",
    "    YEAR,\n",
    "    MONTH\n",
    ")\n",
    "PARTITIONED ON (YEAR, MONTH)\n",
    "AS SELECT\n",
    "    Id,\n",
    "    PostTypeId,\n",
    "    ParentId,\n",
    "    regexp_extract(CreationDate, \"[0-9]{4}\", 0),\n",
    "    regexp_extract(CreationDate, \"[0-9]{4}-[0-9]{2}\", 0)\n",
    "FROM posts_sample;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a query.hql\n",
    "\n",
    "SELECT YEAR, MONTH, count FROM (\n",
    "    SELECT * FROM (\n",
    "        SELECT \n",
    "            YEAR, \n",
    "            MONTH, \n",
    "            count(*) as count, \n",
    "            row_number() over() as rn \n",
    "        FROM posts_sample_view \n",
    "        GROUP BY YEAR, MONTH \n",
    "        ORDER BY rn DESC \n",
    "        LIMIT 3\n",
    "    ) AS t \n",
    "    ORDER BY t.rn ASC \n",
    "    LIMIT 1\n",
    ") AS tt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 1.056 seconds\n",
      "OK\n",
      "Time taken: 0.338 seconds\n",
      "OK\n",
      "Time taken: 0.027 seconds\n",
      "OK\n",
      "Time taken: 0.097 seconds\n",
      "OK\n",
      "Time taken: 0.621 seconds\n",
      "OK\n",
      "Time taken: 0.025 seconds\n",
      "OK\n",
      "Time taken: 1.098 seconds\n",
      "Query ID = jovyan_20180609064848_82ebe5a2-3874-422f-ac35-9570141a428c\n",
      "Total jobs = 4\n",
      "Launching Job 1 out of 4\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1528502025591_0006, Tracking URL = http://a9586ec689fd:8088/proxy/application_1528502025591_0006/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1528502025591_0006\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2018-06-09 06:48:24,404 Stage-1 map = 0%,  reduce = 0%\n",
      "2018-06-09 06:48:42,980 Stage-1 map = 27%,  reduce = 0%, Cumulative CPU 17.91 sec\n",
      "2018-06-09 06:48:48,334 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 24.0 sec\n",
      "2018-06-09 06:48:54,758 Stage-1 map = 62%,  reduce = 0%, Cumulative CPU 30.01 sec\n",
      "2018-06-09 06:48:56,905 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 31.83 sec\n",
      "2018-06-09 06:49:05,514 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 35.3 sec\n",
      "MapReduce Total cumulative CPU time: 35 seconds 300 msec\n",
      "Ended Job = job_1528502025591_0006\n",
      "Launching Job 2 out of 4\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1528502025591_0007, Tracking URL = http://a9586ec689fd:8088/proxy/application_1528502025591_0007/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1528502025591_0007\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2018-06-09 06:49:22,133 Stage-2 map = 0%,  reduce = 0%\n",
      "2018-06-09 06:49:29,705 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.33 sec\n",
      "2018-06-09 06:49:39,374 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.07 sec\n",
      "MapReduce Total cumulative CPU time: 8 seconds 70 msec\n",
      "Ended Job = job_1528502025591_0007\n",
      "Launching Job 3 out of 4\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1528502025591_0008, Tracking URL = http://a9586ec689fd:8088/proxy/application_1528502025591_0008/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1528502025591_0008\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2018-06-09 06:49:55,846 Stage-3 map = 0%,  reduce = 0%\n",
      "2018-06-09 06:50:03,333 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.61 sec\n",
      "2018-06-09 06:50:11,891 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.29 sec\n",
      "MapReduce Total cumulative CPU time: 6 seconds 290 msec\n",
      "Ended Job = job_1528502025591_0008\n",
      "Launching Job 4 out of 4\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1528502025591_0009, Tracking URL = http://a9586ec689fd:8088/proxy/application_1528502025591_0009/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1528502025591_0009\n",
      "Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1\n",
      "2018-06-09 06:50:29,557 Stage-4 map = 0%,  reduce = 0%\n",
      "2018-06-09 06:50:37,045 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.17 sec\n",
      "2018-06-09 06:50:45,647 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.94 sec\n",
      "MapReduce Total cumulative CPU time: 6 seconds 940 msec\n",
      "Ended Job = job_1528502025591_0009\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 35.3 sec   HDFS Read: 60006515 HDFS Write: 3408 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 8.07 sec   HDFS Read: 9223 HDFS Write: 3509 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 6.29 sec   HDFS Read: 7836 HDFS Write: 192 SUCCESS\n",
      "Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 6.94 sec   HDFS Read: 5276 HDFS Write: 16 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 56 seconds 600 msec\n",
      "OK\n",
      "2008\t2008-10\t73\n",
      "Time taken: 158.313 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query.hql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
